{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Portada](https://github.com/ispc-programador2022/FFNFF5/blob/main/WScrap.png?raw=true \"Proyecto Tecnologico Integrador ISPC 2022\")\n",
    "## Importación de Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kp51uIv-IaR_"
   },
   "outputs": [],
   "source": [
    "#Librerias para webScrapping\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de Funciones Principales Scrap1\n",
    "Estas funciones son las que usaremos de manera principal, en ellas estamos declarando las variables que usaremos en el web scraping y además seteamos el formato en que se importaran los archivos con los que vamos a trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wTmcP9AIfU0"
   },
   "outputs": [],
   "source": [
    "def tablaPrincipales(u,a):\n",
    "  \n",
    "  source = urllib.request.urlopen(u).read()\n",
    "  soup = BeautifulSoup(source,'lxml')\n",
    "  table = soup.find_all('table')[0]\n",
    "  df = pd.read_html(str(table), flavor='bs4', header=[0])[0]\n",
    "  df.drop([\"Unnamed: 0\"],axis=1, inplace=True)\n",
    "  if a == \"races\":\n",
    "     df.drop([\"Unnamed: 7\"],axis=1, inplace=True)\n",
    "  elif a == \"drivers\":\n",
    "     df.drop([\"Unnamed: 6\"],axis=1, inplace=True)\n",
    "  elif a == \"team\":\n",
    "     df.drop([\"Unnamed: 4\"],axis=1, inplace=True)\n",
    "  elif a == \"fastest-laps\":\n",
    "     df.drop([\"Unnamed: 5\"],axis=1, inplace=True) \n",
    "  return df\n",
    "\n",
    "def archivos_principales(url,c,n,b):\n",
    "  print(f\"EXPORTANDO...\")\n",
    "  a =\".xlsx\"       \n",
    "  if c == 0:\n",
    "    df = tablaPrincipales(url,n)\n",
    "    x = b+\"_\"+n+\"Carreras\"+a\n",
    "    df.to_excel(x)\n",
    "    print(f\"listo: {x}\")\n",
    "  elif cont == 1:\n",
    "    df = tablaPrincipales(url,n)\n",
    "    x = b+\"_\"+n+\"1Pilotos\"+a\n",
    "    df.to_excel(x)\n",
    "    print(f\"listo: {x}\")\n",
    "  elif cont == 2:\n",
    "    df = tablaPrincipales(url,n)\n",
    "    x = b+\"_\"+n+\"1Equipos\"+a\n",
    "    df.to_excel(x)\n",
    "    print(f\"listo: {x}\")\n",
    "  elif cont == 3:\n",
    "    df = tablaPrincipales(url,n)\n",
    "    x = b+\"_\"+n+\"1Vueltas\"+a\n",
    "    df.to_excel(x)\n",
    "    print(f\"listo: {x}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap1\n",
    "A continuación realizamos el \"scrapeo\" de los archivos principales que usaremos a posterior como tablas principales en nuestro modelo de datos, la obtención de archivos la definimos para los años 2021 y 2022, para chequear el comportamiento del codigo y luego de un analisis rapido ver de cual de ellos podemos disponer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJySc3W8IhmC",
    "outputId": "ba74fb19-d90b-452a-f6e2-8611d89fe418"
   },
   "outputs": [],
   "source": [
    "#Genero los archivos general de los años 2021 y 2022, los exporto\n",
    "nombres=[\"races\",\"drivers\",\"team\",\"fastest-laps\"]\n",
    "fecha=[\"2021\",\"2022\"]\n",
    "cont2 = 0\n",
    "while cont2 < len(fecha):\n",
    "  #print(fecha[cont2])\n",
    "  cont = 0\n",
    "  print(f\"GENRALES{fecha[cont2]}\")\n",
    "  for nom in nombres:\n",
    "    url9=\"https://www.formula1.com/en/results.html/anno/x.html\"\n",
    "    n = nom\n",
    "    b = fecha[cont2]\n",
    "    url9 = url9.replace(\"x\",n)\n",
    "    url9 = url9.replace(\"anno\",b)\n",
    "    #print(url9)\n",
    "    #print(b)\n",
    "    #print(cont)\n",
    "    archivos_principales(url9,cont,nom,b)\n",
    "    cont += 1\n",
    "\n",
    "  cont2 += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de Funciones Principales Scrap2\n",
    "Luego de obtener los archivos principales y posterior a un rapido analisis visual, decidimos de trabajar con los archivos correspondientes al año 2021, ya que en el periodo actual, se encuentra con menos datos al faltar completarse el calendario de carreras del corriente año.\n",
    "\n",
    "Aqui se definen las funciones que nos ayudaran a obtener una mayor profundidad de detalle, en lo que a eventos de la competición se corresponde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvVWnjn0WHw3"
   },
   "outputs": [],
   "source": [
    "#FUNCIONES PARA EL SCRAPEO Y ARMADO DE TABLAS DE CADA CARRERA\n",
    "\n",
    "def tabla1(u):\n",
    "  source = urllib.request.urlopen(u).read()\n",
    "  soup = BeautifulSoup(source,'lxml')\n",
    "  table = soup.find_all('table')[0]\n",
    "  df = pd.read_html(str(table), flavor='bs4', header=[0])[0]\n",
    "  df.drop([\"Unnamed: 0\"],axis=1, inplace=True)\n",
    "  df.drop([\"Unnamed: 8\"],axis=1, inplace=True)\n",
    "  return df\n",
    "\n",
    "\n",
    "def tabla2(u):\n",
    "  source = urllib.request.urlopen(u).read()\n",
    "  soup = BeautifulSoup(source,'lxml')\n",
    "  table = soup.find_all('table')[0]\n",
    "  df = pd.read_html(str(table), flavor='bs4', header=[0])[0]\n",
    "  df.drop([\"Unnamed: 0\"],axis=1, inplace=True)\n",
    "  df.drop([\"Unnamed: 9\"],axis=1, inplace=True)\n",
    "  return df\n",
    "\n",
    "\n",
    "def tabla3(u):\n",
    "  source = urllib.request.urlopen(u).read()\n",
    "  soup = BeautifulSoup(source,'lxml')\n",
    "  table = soup.find_all('table')[0]\n",
    "  df = pd.read_html(str(table), flavor='bs4', header=[0])[0]\n",
    "  df.drop([\"Unnamed: 0\"],axis=1, inplace=True)\n",
    "  #df.drop([\"Unnamed: 6\"],axis=1, inplace=True)\n",
    "  return df\n",
    "\n",
    "def archivos(url,url2,url3,url4,url5,n):\n",
    "  cont = 0\n",
    "  a =\".xlsx\"\n",
    "  while cont <5:\n",
    "        cont += 1\n",
    "        if cont == 1:\n",
    "          df = tabla1(url)\n",
    "          x = n+\"1_race-result\"+a\n",
    "          df.to_excel(x)\n",
    "        elif cont == 2:\n",
    "          df = tabla2(url2)\n",
    "          x = n+\"2_fastest-laps\"+a\n",
    "          df.to_excel(x)\n",
    "        elif cont == 3:\n",
    "          df = tabla2(url3)\n",
    "          x = n+\"3_pit-stop\"+a\n",
    "          df.to_excel(x)\n",
    "        elif cont == 4:\n",
    "          df = tabla3(url4)\n",
    "          x = n+\"4_starting-grid\"+a\n",
    "          df.to_excel(x)\n",
    "        elif cont == 5:\n",
    "          df = tabla2(url5)\n",
    "          x = n+\"5_qualifying\"+a\n",
    "          df.to_excel(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap2\n",
    "A continuacion realizamos los arreglos necesarios para extraer de la web toda la información para trabajar, indicando las url destino a las cual apuntar. Por ultimo mostramos la extración de las tablas y la confirmación de descarga de las mismas.\n",
    "**NOTA:** Se excluye la información correspondiente al Premio de Belgica, ya que dicha carrera fue suspendida luego de 3 vueltas por las inclemencias del tiempo. [Ver info.](https://soymotor.com/resultados/gp-de-belgica-f1-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvNlkP1fFtyW",
    "outputId": "b13d7c04-dbc9-4a4f-9385-0ee864299ce1"
   },
   "outputs": [],
   "source": [
    "#creamos un arreglo con los ID  de las carreras a buscar en la pagina web \n",
    "indice=[\"1064\",\t\"1065\",\t\"1066\",\t\"1086\",\t\"1067\",\t\"1068\",\t\"1070\",\t\"1092\",\t\"1071\",\t\"1072\",\t\n",
    "        \"1073\",\t\"1075\",\t\"1076\",\t\"1077\",\t\"1078\",\t\"1102\",\t\"1103\",\t\"1104\",\t\"1105\",\t\n",
    "        \"1106\",\t\"1107\"]\n",
    "#creamos un arreglo con los nombres de las carreras a buscar en la pagina web\n",
    "carreras=[\"bahrain\",\"italy1\",\"portugal\",\"spain\",\"monaco\",\"azerbaijan\",\"france\",\"austria1\",\"austria2\",\"great-britain\",\"hungary\",\"netherlands\",\"italy2\",\"russia\",\"turquia\",\"united-states\",\"mexico\",\"brasil\",\"qatar\",\"saudi-arabia\",\"abu-dhabi\"]\n",
    "\n",
    "#inicializamos el contador para  utilizarlo para buscar en el indice de ID       \n",
    "cont=0\n",
    "for carrera in carreras:\n",
    "  #DIRECCIONES SCRAPEAR\n",
    "  url = \"https://www.formula1.com/en/results.html/2021/races/1064/x/race-result.html\"\n",
    "  url2 = \"https://www.formula1.com/en/results.html/2021/races/1064/x/fastest-laps.html\"\n",
    "  url3 = \"https://www.formula1.com/en/results.html/2021/races/1064/x/pit-stop-summary.html\"\n",
    "  url4 = \"https://www.formula1.com/en/results.html/2021/races/1064/x/starting-grid.html\"\n",
    "  url5 = \"https://www.formula1.com/en/results.html/2021/races/1064/x/qualifying.html\"\n",
    " \n",
    "  n = carrera #CARRERA QUE BUSCAMOS\n",
    "  #print(f\"-------{n}-{indice[cont]}----{cont}\")\n",
    "  #CREAMOS LA URL CON LOS DATOS QUE QUEREMOS PARA BUSCAR LOS DATOS - \n",
    "  url = url.replace(\"x\",n) #reemplazamos x por la carrera que buscamos  \n",
    "  url = url.replace(\"1064\",indice[cont]) #reemplazamos el id por el de la carrera que buscamos\n",
    "  #print(url) #Prueba de url\n",
    "  url2 = url2.replace(\"x\",n)\n",
    "  url2 = url2.replace(\"1064\",indice[cont])\n",
    "  #print(url2)\n",
    "  url3 = url3.replace(\"x\",n)\n",
    "  url3 = url3.replace(\"1064\",indice[cont])\n",
    "  #print(url3)\n",
    "  url4 = url4.replace(\"x\",n)\n",
    "  url4 = url4.replace(\"1064\",indice[cont])\n",
    "  #print(url4)\n",
    "  url5 = url5.replace(\"x\",n)\n",
    "  url5 = url5.replace(\"1064\",indice[cont])\n",
    "  #print(url5)\n",
    "  \n",
    "  cont += 1 #sumanmos uno para seguir recorriendo el indice\n",
    "  archivos(url,url2,url3,url4,url5,n) #crea el DF de cada carrera y lo exporta \n",
    "  print(f\"----ARCHIVO CREADO---{n}-{cont}\") #comprobamos los archivos creados\n",
    "  print(\"-------------------------------------\")\n",
    "\n",
    "print(f\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")\n",
    "print(f\"* Operacion exitosa- Archivos  creados {cont} *\")\n",
    "print(f\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la notebook siguiente, realizaremos un pre-procesado de los archivos \"Scrapeados\" y la posterior carga en Base de Datos de MySQL."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
